# ğŸ” Agentes de Aprendizaje por Refuerzo en un Laberinto

## ğŸ¤ Colaboradores

- [Jfriera03](https://github.com/Jfriera03)
- [MasoalM](https://github.com/MasoalM)

## ğŸ“Œ DescripciÃ³n

Este proyecto implementa agentes inteligentes basados en **aprendizaje por refuerzo** para resolver un laberinto. Se desarrollan estrategias mediante **Q-Learning** y **SARSA**, optimizando el comportamiento de los agentes en un entorno dinÃ¡mico.

## ğŸ› ï¸ TecnologÃ­as y Herramientas

- **Lenguaje:** Python ğŸ
- **LibrerÃ­as utilizadas:** `numpy`, `random`, `logging`, `pygame`.
- **Estructuras de datos empleadas:**
  - Diccionarios para almacenar valores Q.
  - Tablas de valores para las estrategias de refuerzo.
  - Algoritmos de actualizaciÃ³n de polÃ­tica (Îµ-greedy, aprendizaje temporal).

## ğŸ“Œ Arquitectura del CÃ³digo

El cÃ³digo estÃ¡ estructurado en los siguientes archivos:

- **`__main__.py`**: Punto de entrada del programa, inicializa el entorno y entrena a los agentes.
- **`agent.py`**: ImplementaciÃ³n base de un agente con aprendizaje por refuerzo.
- **`agentSARSA.py`**: ImplementaciÃ³n del agente usando el algoritmo **SARSA**.
- **`abstractmodel.py`**: Clase base abstracta para modelos de predicciÃ³n y entrenamiento.
- **`joc.py`**: Define la estructura del laberinto y las reglas del juego.

## ğŸš€ InstalaciÃ³n y Uso

1. Clona el repositorio:
   ```bash
   git clone https://github.com/Jfriera03/agentes-refuerzo.git
   ```
2. Accede al directorio del proyecto:
   ```bash
   cd agentes-refuerzo
   ```
3. Instala las dependencias necesarias:
   ```bash
   pip install -r requirements.txt
   ```
4. Ejecuta el cÃ³digo principal:
   ```bash
   python __main__.py
   ```

## ğŸ® Funcionamiento del Algoritmo

El entorno es un tablero de **8x8** donde un agente debe encontrar el camino Ã³ptimo hasta la meta evitando obstÃ¡culos. Las acciones disponibles son:

- **MOVE UP** â†’ Moverse hacia arriba.
- **MOVE DOWN** â†’ Moverse hacia abajo.
- **MOVE LEFT** â†’ Moverse hacia la izquierda.
- **MOVE RIGHT** â†’ Moverse hacia la derecha.

El agente aprende a travÃ©s de ensayo y error, actualizando su estrategia con cada episodio de entrenamiento.

### ğŸ”¹ Q-Learning
- Aprende mediante la ecuaciÃ³n de Bellman.
- Utiliza **exploraciÃ³n-explotaciÃ³n** para mejorar decisiones.
- Almacena valores en una **tabla Q** para cada estado y acciÃ³n.

### ğŸ”¹ SARSA
- Similar a Q-Learning, pero actualiza valores considerando la acciÃ³n siguiente.
- Permite estrategias mÃ¡s conservadoras en entornos dinÃ¡micos.
- Utiliza **Îµ-greedy** para ajustar la exploraciÃ³n.

## ğŸ—ï¸ ImplementaciÃ³n TÃ©cnica

### ğŸ“Œ Estados y PercepciÃ³n
Cada agente recibe informaciÃ³n del entorno a travÃ©s de un diccionario:
- **`POS`**: PosiciÃ³n actual del agente en el laberinto.
- **`MAZE`**: Matriz de celdas con obstÃ¡culos y caminos libres.
- **`EXIT`**: Coordenadas de la casilla objetivo.

### ğŸ“Œ Clases Principales

- **`AbstractModel`**: Clase base para modelos de aprendizaje.
- **`AgentQ`**: ImplementaciÃ³n de **Q-Learning**.
- **`AgentSARSA`**: ImplementaciÃ³n de **SARSA**.
- **`Laberint`**: Controla el entorno y aplica reglas del juego.

### ğŸ“Œ Recompensas y Penalizaciones
El entrenamiento se basa en un sistema de recompensas:
- **Recompensa positiva** (+10) al alcanzar el objetivo.
- **PenalizaciÃ³n** (-0.05) por cada movimiento sin Ã©xito.
- **PenalizaciÃ³n alta** (-0.75) por intentar moverse a una pared.

Los algoritmos ajustan sus decisiones para maximizar la recompensa acumulada.
